{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8de9ac2-c7c5-4e43-86c7-cd160d761a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hm/psq79zp10blbxnxgz7xxgx0r0000gn/T/ipykernel_6825/2880966951.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b307e231-1fc1-43ea-9148-16ed4c82b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2ef165-798f-42b9-877b-028e8a01e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a466760c-7313-4fba-928d-2e1e6948df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file '/Users/naziultalukder/.matplotlib/matplotlibrc', line 2 ('backend: PyQt5')\n",
      "Duplicate key in file '/Users/naziultalukder/.matplotlib/matplotlibrc', line 3 ('backend: TkAgg')\n"
     ]
    }
   ],
   "source": [
    "from pyexpat import features\n",
    "import copy\n",
    "import math\n",
    "from sys import prefix\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import copy\n",
    "\n",
    "from transformers import CLIPModel, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abfcec5d-d901-4486-92ca-26344a46c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import CLIPTokenizer, CLIPProcessor, AutoTokenizer\n",
    "\n",
    "\n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, root_folder, image_folder, split='train', labels='original', image_size=224):\n",
    "        super(HatefulMemesDataset, self).__init__()\n",
    "        self.root_folder = root_folder\n",
    "        self.image_folder = image_folder\n",
    "        self.split = split\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        self.info_file = f\"{self.split}.csv\"\n",
    "\n",
    "        print(\"data here: \", self.info_file)\n",
    "        self.df = pd.read_csv(self.info_file)\n",
    "        # float_cols = self.df.select_dtypes(float).columns\n",
    "        # self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {}\n",
    "        image_fn = row['img'].split('/')[1]\n",
    "        item['image'] = Image.open(f\"{self.image_folder}/{image_fn}\").convert('RGB').resize((self.image_size, self.image_size))\n",
    "        item['text'] = row['text']\n",
    "        item['label'] = row['label']\n",
    "        item['idx_meme'] = row['id']\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7231e13-89db-4750-913b-3907df9bafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pre_trained_model = 'openai/clip-vit-base-patch32'\n",
    "        self.image_processor = CLIPProcessor.from_pretrained(pre_trained_model)\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained(pre_trained_model)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = self.image_processor(images=[item['image'] for item in batch], return_tensors=\"pt\")['pixel_values']\n",
    "        text_output = self.text_processor([item['text'] for item in batch], padding=True, return_tensors=\"pt\", truncation=True)\n",
    "        labels = torch.LongTensor([item['label'] for item in batch])\n",
    "        idx_memes = torch.LongTensor([item['idx_meme'] for item in batch])\n",
    "\n",
    "        batch_new = {}\n",
    "        batch_new['pixel_values'] = pixel_values,\n",
    "        batch_new['input_ids'] = text_output['input_ids']\n",
    "        batch_new['attention_mask'] = text_output['attention_mask']\n",
    "\n",
    "        batch_new['labels'] = labels\n",
    "        batch_new['idx_memes'] = idx_memes\n",
    "\n",
    "        return batch_new\n",
    "\n",
    "collator = CustomCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ab19eab-845b-4ad0-ae62-28023bbdfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores\n",
    "multilingual_tokenizer_path = 'none'\n",
    "fine_grained_labels = []\n",
    "compute_fine_grained_metrics = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2bece-bf7f-47d2-9db5-1e01e5b4c6ef",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9a0dba-8b31-47f7-8ca5-baca8d773e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data here:  train.csv\n",
      "data here:  dev.csv\n"
     ]
    }
   ],
   "source": [
    "labels = \"original\"\n",
    "image_folder = 'data/img'\n",
    "image_size = 224\n",
    "\n",
    "dataset_train = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, split='train',\n",
    "            labels=labels, image_size=image_size)\n",
    "\n",
    "dataset_val = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, split='dev',\n",
    "            labels=labels, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217b521-0ea1-4e16-abd1-4f68d7f66bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc10f82-8260-4941-9ef5-a4d2f54402c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=224x224>,\n",
       " 'text': 'its their character not their color that matters',\n",
       " 'label': 0,\n",
       " 'idx_meme': 42953}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f34994-b05d-47a8-8cf2-1fa2fdd7635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=0, collate_fn=collator)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False, num_workers=0, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61651d53-9b65-4144-a878-7004a9a1ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fine_grained_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a082520f-e300-47ce-a50f-701e9fbd9351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2c840d8b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80d1c-b49c-4838-9df9-3e70b3a00fd2",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fc288bb-4726-4044-b021-0f27735ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, fine_grained_labels, compute_fine_grained_metrics):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.caption_mode = args.caption_mode\n",
    "        self.use_pretrained_map = args['use_pretrained_map']\n",
    "        self.num_mapping_layers = args['num_mapping_layers']\n",
    "        self.map_dim = args['map_dim']\n",
    "        self.fusion = args['fusion']\n",
    "        self.num_pre_output_layers = args['num_pre_output_layers']\n",
    "        self.lr = args['lr']\n",
    "        self.weight_decay = args['weight_decay']\n",
    "        self.weight_image_loss = args['weight_image_loss']\n",
    "        self.weight_text_loss = args['weight_text_loss']\n",
    "        self.weight_fine_grained_loss = args['weight_fine_grained_loss']\n",
    "        self.weight_super_loss = args['weight_super_loss']\n",
    "        self.fine_grained_labels = fine_grained_labels\n",
    "        self.compute_fine_grained_metrics = compute_fine_grained_metrics\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.auroc = torchmetrics.AUROC(task=\"binary\")\n",
    "        # self.precision_score = torchmetrics.Precision()\n",
    "        # self.recall = torchmetrics.Recall()\n",
    "        # self.f1 = torchmetrics.F1Score()\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "       \n",
    "\n",
    "        self.clip = CLIPModel.from_pretrained(args['clip_pretrained_model'])\n",
    "        self.image_encoder = copy.deepcopy(self.clip.vision_model)\n",
    "        self.text_encoder = copy.deepcopy(self.clip.text_model)\n",
    "        self.image_map = nn.Sequential(\n",
    "                copy.deepcopy(self.clip.visual_projection),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "                )\n",
    "        self.text_map = nn.Sequential(\n",
    "            copy.deepcopy(self.clip.text_projection),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if args['fusion'] in ['align', 'align_shuffle']:\n",
    "            pre_output_input_dim = self.map_dim\n",
    "        elif args['fusion'] == 'concat':\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "        elif args['fusion'].startswith('cross'):\n",
    "            pre_output_input_dim = self.map_dim**2\n",
    "        elif args['fusion'] == 'align_concat':\n",
    "            pre_output_input_dim = self.map_dim*3\n",
    "        elif args['fusion'] == 'attention_m':\n",
    "            self.gen_query = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.gen_key = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.soft = nn.Softmax(dim=1)\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "\n",
    "        pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "        output_input_dim = pre_output_input_dim\n",
    "\n",
    "\n",
    "        if self.num_pre_output_layers >= 1: # first pre-output layer\n",
    "            pre_output_layers.extend([nn.Linear(pre_output_input_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            output_input_dim = self.map_dim\n",
    "        for _ in range(1, self.num_pre_output_layers): # next pre-output layers\n",
    "            pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.output = nn.Linear(output_input_dim, 1)\n",
    "        self.output_image = nn.Linear(output_input_dim, 1)\n",
    "        self.output_text = nn.Linear(output_input_dim, 1)\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_image = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_text = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.fine_grained_labels:\n",
    "            # if self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            self.output_pc1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack7 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack8 = nn.Linear(output_input_dim, 1)\n",
    "            self.outputs_fine_grained = [self.output_pc1, self.output_pc2, self.output_pc3, self.output_pc4, self.output_pc5, self.output_pc6,\n",
    "                self.output_attack1, self.output_attack2, self.output_attack3, self.output_attack4, self.output_attack5, self.output_attack6, self.output_attack7, self.output_attack8]\n",
    "            self.output_super = nn.Linear(15, 1)\n",
    "\n",
    "        self.cross_entropy_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        if args['freeze_image_encoder']:\n",
    "            for _, p in self.image_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        if args['freeze_text_encoder']:\n",
    "            for _, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        del self.clip\n",
    "        # if self.caption_mode == 'replace_image':\n",
    "        #     del self.image_encoder, self.image_map\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1) # [batch_size, d]\n",
    "        text_features = F.normalize(text_features, p=2, dim=1) # [batch_size, d]\n",
    "\n",
    "        features = torch.mul(image_features, text_features)  # [batch_size, d]\n",
    "\n",
    "        features = self.pre_output(features)\n",
    "        logits = self.output(features)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def common_step(self, batch, batch_idx, calling_function='validation'):\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "        text_features = self.text_map(text_features)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            features_pre_output = self.pre_output_image(image_features)\n",
    "            logits = self.output_image(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['image_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['image_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['image_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            features_pre_output = self.pre_output_text(text_features)\n",
    "            logits = self.output_text(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['text_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['text_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['text_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        features = torch.mul(image_features, text_features)\n",
    "\n",
    "        features_pre_output = self.pre_output(features)\n",
    "        logits = self.output(features_pre_output).squeeze(dim=1) # [batch_size, 1(or)n]\n",
    "        if self.fine_grained_labels and self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            logits_for_super = [torch.relu(logits)]\n",
    "        preds_proxy = torch.sigmoid(logits)\n",
    "        preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
    "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "\n",
    "        if calling_function == 'training' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output['super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        elif calling_function == 'validation' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "                output[f'{fine_grained_label}_accuracy'] = self.acc(preds, batch[fine_grained_label])\n",
    "                output[f'{fine_grained_label}_auroc'] = self.auroc(preds_proxy, batch[fine_grained_label])\n",
    "                \n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output[f'super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output[f'super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output[f'super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        elif calling_function == 'visualisation-v1':\n",
    "            return image_features, text_features\n",
    "\n",
    "        elif calling_function == 'visualisation-v2':\n",
    "            return features\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='training')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = 0\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                fine_grained_loss += output[f'{fine_grained_label}_loss']\n",
    "            fine_grained_loss /= len(self.fine_grained_labels)\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log('train/total_loss', total_loss)\n",
    "        self.log('train/loss', output['loss'])\n",
    "        self.log('train/accuracy', output['accuracy'])\n",
    "        self.log('train/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log('train/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log('train/text_loss', text_loss)\n",
    "\n",
    "        self.log('train/fine_grained_loss', fine_grained_loss)\n",
    "        self.log('train/super_loss', super_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='validation')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = torch.mean(torch.Tensor([output[f'{fine_grained_label}_loss'] for fine_grained_label in self.fine_grained_labels]))\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'])\n",
    "        self.log(f'val/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log(f'val/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log(f'val/text_loss', text_loss)\n",
    "\n",
    "\n",
    "        # TODO include this logic if needed\n",
    "        if self.fine_grained_labels and self.compute_fine_grained_metrics:\n",
    "            self.log(f'val/fine_grained_loss', fine_grained_loss)\n",
    "            self.log(f'val/super_loss', super_loss)\n",
    "\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_accuracy', output[f'{fine_grained_label}_accuracy'])\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_auroc', output[f'{fine_grained_label}_auroc'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_precision', output[f'{fine_grained_label}_precision'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_recall', output[f'{fine_grained_label}_recall'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_f1', output[f'{fine_grained_label}_f1'])\n",
    "\n",
    "            self.log(f'val/super_loss', output['super_loss'])\n",
    "            self.log(f'val/super_accuracy', output['super_accuracy'])\n",
    "            self.log(f'val/super_auroc', output['super_auroc'])\n",
    "\n",
    "        self.validation_step_outputs.append(total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    # def on_train_epoch_end(self, validation_step_outputs):\n",
    "    def on_train_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "    # def on_validation_epoch_end(self, validation_step_outputs):\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    # def test_epoch_end(self, validation_step_outputs):\n",
    "    #     self.acc.reset()\n",
    "    #     self.auroc.reset()\n",
    "    #     self.precision_score.reset()\n",
    "    #     self.recall.reset()\n",
    "    #     self.f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
    "            ]\n",
    "        # print(\"what are params \", param_dicts)\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def create_model(args, fine_grained_labels):\n",
    "    compute_fine_grained_metrics = True\n",
    "    model = CLIPClassifier(args=args, fine_grained_labels=fine_grained_labels, compute_fine_grained_metrics = compute_fine_grained_metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23be74d9-efa7-41dc-a23a-c1af110f277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup params\n",
    "clip_model = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e4,\n",
    "    \"weight_image_loss\": 1.0,\n",
    "    \"weight_text_loss\": 1.0,\n",
    "    \"weight_fine_grained_loss\": 1.0,\n",
    "    \"weight_super_loss\": 1.0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e4,\n",
    "    \"weight_image_loss\": 1.0,\n",
    "    \"weight_text_loss\": 1.0,\n",
    "    \"weight_fine_grained_loss\": 1.0,\n",
    "    \"weight_super_loss\": 1.0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "\n",
    "model = create_model(default_param, fine_grained_labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b40b823-a22b-4803-aea3-7629ebb13664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (acc): BinaryAccuracy()\n",
       "  (auroc): BinaryAUROC()\n",
       "  (image_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_image): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_text): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (pre_output_image): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pre_output_text): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cross_entropy_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cddf080-0c0d-45da-86fc-499d7223928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# monitor=\"val/auroc\"\n",
    "# project=\"meme-v2\"\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename=project,  monitor=monitor, \n",
    "#                                       mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a80e2a0-5d6c-48ad-b65b-ba9bc8cbf57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "seed_everything(42)\n",
    "model = create_model(default_param, fine_grained_labels=[])\n",
    "\n",
    "\n",
    "# #TODO add GPU later\n",
    "# gpus=args.gpus\n",
    "\n",
    "max_steps = -1\n",
    "gradient_clip_val = 0.1\n",
    "log_every_n_steps = 50\n",
    "max_epochs = -1\n",
    "val_check_interval = 1.0\n",
    "limit_train_batches = 1.0\n",
    "limit_val_batches = 1.0\n",
    "\n",
    "monitor=\"val/auroc\"\n",
    "project=\"meme-v2\"\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename='checkpointFile',  \n",
    "                                      monitor=monitor, mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)\n",
    "\n",
    "# accelerator=\"cpu\", devices=2\n",
    "# accelerator=\"gpu\", devices=1\n",
    "\n",
    "trainer = Trainer(max_epochs=max_epochs, max_steps=max_steps, gradient_clip_val=gradient_clip_val, \n",
    "        log_every_n_steps=log_every_n_steps, val_check_interval=val_check_interval, accelerator=\"cpu\", devices=1,\n",
    "        strategy=\"auto\", callbacks=[checkpoint_callback],\n",
    "        limit_train_batches=limit_train_batches, limit_val_batches=limit_val_batches,\n",
    "        deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebd455-0b16-4c0a-a44a-dbca8e9566f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory checkpoints exists and is not empty.\n",
      "\n",
      "   | Name               | Type                  | Params\n",
      "--------------------------------------------------------------\n",
      "0  | acc                | BinaryAccuracy        | 0     \n",
      "1  | auroc              | BinaryAUROC           | 0     \n",
      "2  | image_encoder      | CLIPVisionTransformer | 87.5 M\n",
      "3  | text_encoder       | CLIPTextTransformer   | 63.2 M\n",
      "4  | image_map          | Sequential            | 787 K \n",
      "5  | text_map           | Sequential            | 656 K \n",
      "6  | pre_output         | Sequential            | 590 K \n",
      "7  | output             | Linear                | 769   \n",
      "8  | output_image       | Linear                | 769   \n",
      "9  | output_text        | Linear                | 769   \n",
      "10 | pre_output_image   | Sequential            | 590 K \n",
      "11 | pre_output_text    | Sequential            | 590 K \n",
      "12 | cross_entropy_loss | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "150 M     Non-trainable params\n",
      "153 M     Total params\n",
      "615.357   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                            …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ebd7c06e2f4eb0bddb62ec71bdb243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naziultalukder/miniconda3/envs/test-env4/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Epoch 0, global step 532: 'val/auroc' reached 0.44800 (best 0.44800), saving model to 'checkpoints/checkpointFile-v3.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1064: 'val/auroc' reached 0.44800 (best 0.44800), saving model to 'checkpoints/checkpointFile-v4.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1596: 'val/auroc' reached 0.44800 (best 0.44800), saving model to 'checkpoints/checkpointFile-v5.ckpt' as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 2128: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2660: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 3192: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 3724: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 4256: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 4788: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 5320: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 5852: 'val/auroc' was not in top 3\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc65d5-3f56-4a7c-9dcf-7c4a4bcf0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b45cd-3c24-4cf6-83c6-556e0f7d50e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505607b-c204-4047-8376-5a3f859c3762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b73bd44-dbb2-4f9a-98ec-7d55662cccd0",
   "metadata": {},
   "source": [
    "- Added 0 worker for dataloader and 1 device for trainer. The auroc is 0.448 and it is unchanged through different epochs.\n",
    "- <the issue could be that configure_optimizer is not taking the right parameters and updating the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2b165-662a-428d-8e24-0d397cb94de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test4",
   "language": "python",
   "name": "test4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install ftfy regex tqdm\n",
    "# ! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8de9ac2-c7c5-4e43-86c7-cd160d761a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b307e231-1fc1-43ea-9148-16ed4c82b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoemam/anaconda3/envs/clip_prefix_caption/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a2ef165-798f-42b9-877b-028e8a01e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a466760c-7313-4fba-928d-2e1e6948df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import features\n",
    "import copy\n",
    "import math\n",
    "from sys import prefix\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import copy\n",
    "\n",
    "from transformers import CLIPModel, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abfcec5d-d901-4486-92ca-26344a46c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import CLIPTokenizer, CLIPProcessor, AutoTokenizer\n",
    "\n",
    "\n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, root_folder, image_folder, split='train', labels='original', image_size=224):\n",
    "        super(HatefulMemesDataset, self).__init__()\n",
    "        self.root_folder = root_folder\n",
    "        self.image_folder = image_folder\n",
    "        self.split = split\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        self.info_file = f\"{self.split}.csv\"\n",
    "\n",
    "        print(\"data here: \", self.info_file)\n",
    "        self.df = pd.read_csv(self.info_file)\n",
    "        # float_cols = self.df.select_dtypes(float).columns\n",
    "        # self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        item = {}\n",
    "        image_fn = row['img'].split('/')[1]\n",
    "        item['image'] = Image.open(f\"{self.image_folder}/{image_fn}\").convert('RGB').resize((self.image_size, self.image_size))\n",
    "        item['text'] = row['text']\n",
    "        item['label'] = row['label']\n",
    "        item['idx_meme'] = row['id']\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "# CLIPModel\n",
    "default_param = \"openai/clip-vit-large-patch14\"\n",
    "my_clip = CLIPModel.from_pretrained(default_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7231e13-89db-4750-913b-3907df9bafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pre_trained_model = 'openai/clip-vit-base-patch32'\n",
    "        self.image_processor = CLIPProcessor.from_pretrained(pre_trained_model)\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained(pre_trained_model)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = self.image_processor(images=[item['image'] for item in batch], return_tensors=\"pt\")['pixel_values']\n",
    "        text_output = self.text_processor([item['text'] for item in batch], padding=True, return_tensors=\"pt\", truncation=True)\n",
    "        labels = torch.LongTensor([item['label'] for item in batch])\n",
    "        idx_memes = torch.LongTensor([item['idx_meme'] for item in batch])\n",
    "\n",
    "        batch_new = {}\n",
    "        batch_new['pixel_values'] = pixel_values,\n",
    "        batch_new['input_ids'] = text_output['input_ids']\n",
    "        batch_new['attention_mask'] = text_output['attention_mask']\n",
    "\n",
    "        batch_new['labels'] = labels\n",
    "        batch_new['idx_memes'] = idx_memes\n",
    "\n",
    "        return batch_new\n",
    "\n",
    "collator = CustomCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab19eab-845b-4ad0-ae62-28023bbdfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores\n",
    "multilingual_tokenizer_path = 'none'\n",
    "fine_grained_labels = []\n",
    "compute_fine_grained_metrics = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2bece-bf7f-47d2-9db5-1e01e5b4c6ef",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c9a0dba-8b31-47f7-8ca5-baca8d773e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data here:  train.csv\n",
      "data here:  dev.csv\n"
     ]
    }
   ],
   "source": [
    "labels = \"original\"\n",
    "image_folder = '../data/img'\n",
    "image_size = 224\n",
    "\n",
    "dataset_train = HatefulMemesDataset(root_folder='.', image_folder=image_folder, split='train',\n",
    "            labels=labels, image_size=image_size)\n",
    "\n",
    "dataset_val = HatefulMemesDataset(root_folder='.', image_folder=image_folder, split='dev',\n",
    "            labels=labels, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217b521-0ea1-4e16-abd1-4f68d7f66bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc10f82-8260-4941-9ef5-a4d2f54402c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=224x224>,\n",
       " 'text': 'its their character not their color that matters',\n",
       " 'label': 0,\n",
       " 'idx_meme': 42953}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64f34994-b05d-47a8-8cf2-1fa2fdd7635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=265, shuffle=True, num_workers=15, collate_fn=collator)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=265, shuffle=False, num_workers=15, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61651d53-9b65-4144-a878-7004a9a1ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fine_grained_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a082520f-e300-47ce-a50f-701e9fbd9351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f1d5ab47f40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80d1c-b49c-4838-9df9-3e70b3a00fd2",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fc288bb-4726-4044-b021-0f27735ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, fine_grained_labels, compute_fine_grained_metrics):\n",
    "        super().__init__()\n",
    "        \n",
    "        # focal loss parameters\n",
    "        self.alpha = 0.9\n",
    "        self.gamma = 10\n",
    "\n",
    "        # self.caption_mode = args.caption_mode\n",
    "        self.use_pretrained_map = args['use_pretrained_map']\n",
    "        self.num_mapping_layers = args['num_mapping_layers']\n",
    "        self.map_dim = args['map_dim']\n",
    "        self.fusion = args['fusion']\n",
    "        self.num_pre_output_layers = args['num_pre_output_layers']\n",
    "        self.lr = args['lr']\n",
    "        self.weight_decay = args['weight_decay']\n",
    "        self.weight_image_loss = args['weight_image_loss']\n",
    "        self.weight_text_loss = args['weight_text_loss']\n",
    "        self.weight_fine_grained_loss = args['weight_fine_grained_loss']\n",
    "        self.weight_super_loss = args['weight_super_loss']\n",
    "        self.fine_grained_labels = fine_grained_labels\n",
    "        self.compute_fine_grained_metrics = compute_fine_grained_metrics\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.auroc = torchmetrics.AUROC(task=\"binary\")\n",
    "        # self.precision_score = torchmetrics.Precision()\n",
    "        # self.recall = torchmetrics.Recall()\n",
    "        # self.f1 = torchmetrics.F1Score()\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "       \n",
    "\n",
    "        self.clip = CLIPModel.from_pretrained(args['clip_pretrained_model'])\n",
    "        self.image_encoder = copy.deepcopy(self.clip.vision_model)\n",
    "        self.text_encoder = copy.deepcopy(self.clip.text_model)\n",
    "        self.image_map = nn.Sequential(\n",
    "                copy.deepcopy(self.clip.visual_projection),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "                )\n",
    "        self.text_map = nn.Sequential(\n",
    "            copy.deepcopy(self.clip.text_projection),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if args['fusion'] in ['align', 'align_shuffle']:\n",
    "            pre_output_input_dim = self.map_dim\n",
    "        elif args['fusion'] == 'concat':\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "        elif args['fusion'].startswith('cross'):\n",
    "            pre_output_input_dim = self.map_dim**2\n",
    "        elif args['fusion'] == 'align_concat':\n",
    "            pre_output_input_dim = self.map_dim*3\n",
    "        elif args['fusion'] == 'attention_m':\n",
    "            self.gen_query = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.gen_key = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.soft = nn.Softmax(dim=1)\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "\n",
    "        pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "        output_input_dim = pre_output_input_dim\n",
    "\n",
    "\n",
    "        if self.num_pre_output_layers >= 1: # first pre-output layer\n",
    "            pre_output_layers.extend([nn.Linear(pre_output_input_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            output_input_dim = self.map_dim\n",
    "        for _ in range(1, self.num_pre_output_layers): # next pre-output layers\n",
    "            pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.output = nn.Linear(output_input_dim, 1)\n",
    "        self.output_image = nn.Linear(output_input_dim, 1)\n",
    "        self.output_text = nn.Linear(output_input_dim, 1)\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_image = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_text = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.fine_grained_labels:\n",
    "            # if self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            self.output_pc1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack7 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack8 = nn.Linear(output_input_dim, 1)\n",
    "            self.outputs_fine_grained = [self.output_pc1, self.output_pc2, self.output_pc3, self.output_pc4, self.output_pc5, self.output_pc6,\n",
    "                self.output_attack1, self.output_attack2, self.output_attack3, self.output_attack4, self.output_attack5, self.output_attack6, self.output_attack7, self.output_attack8]\n",
    "            self.output_super = nn.Linear(15, 1)\n",
    "\n",
    "        self.cross_entropy_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        if args['freeze_image_encoder']:\n",
    "            for _, p in self.image_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        if args['freeze_text_encoder']:\n",
    "            for _, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        del self.clip\n",
    "        # if self.caption_mode == 'replace_image':\n",
    "        #     del self.image_encoder, self.image_map\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1) # [batch_size, d]\n",
    "        text_features = F.normalize(text_features, p=2, dim=1) # [batch_size, d]\n",
    "\n",
    "        features = torch.mul(image_features, text_features)  # [batch_size, d]\n",
    "\n",
    "        features = self.pre_output(features)\n",
    "        logits = self.output(features)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def common_step(self, batch, batch_idx, calling_function='validation'):\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "        text_features = self.text_map(text_features)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            features_pre_output = self.pre_output_image(image_features)\n",
    "            logits = self.output_image(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['image_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['image_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['image_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            features_pre_output = self.pre_output_text(text_features)\n",
    "            logits = self.output_text(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['text_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['text_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['text_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        features = torch.mul(image_features, text_features)\n",
    "\n",
    "        features_pre_output = self.pre_output(features)\n",
    "        logits = self.output(features_pre_output).squeeze(dim=1) # [batch_size, 1(or)n]\n",
    "        if self.fine_grained_labels and self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            logits_for_super = [torch.relu(logits)]\n",
    "        preds_proxy = torch.sigmoid(logits)\n",
    "        preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "        # BCE_loss = F.binary_cross_entropy_with_logits(preds.float(), batch['labels'].float(), reduction='none')\n",
    "        # pt = torch.exp(-BCE_loss) # prevents nans when probability 0\n",
    "\n",
    "      \n",
    "        # output['loss'] = torch.mean(torch.Tensor(self.alpha * (1-pt)**self.gamma * BCE_loss))\n",
    "        # output['loss'].requires_grad_()\n",
    "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
    "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "\n",
    "        if calling_function == 'training' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output['super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        elif calling_function == 'validation' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "                output[f'{fine_grained_label}_accuracy'] = self.acc(preds, batch[fine_grained_label])\n",
    "                output[f'{fine_grained_label}_auroc'] = self.auroc(preds_proxy, batch[fine_grained_label])\n",
    "                \n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output[f'super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output[f'super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output[f'super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        elif calling_function == 'visualisation-v1':\n",
    "            return image_features, text_features\n",
    "\n",
    "        elif calling_function == 'visualisation-v2':\n",
    "            return features\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='training')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = 0\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                fine_grained_loss += output[f'{fine_grained_label}_loss']\n",
    "            fine_grained_loss /= len(self.fine_grained_labels)\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log('train/total_loss', total_loss)\n",
    "        self.log('train/loss', output['loss'])\n",
    "        self.log('train/accuracy', output['accuracy'])\n",
    "        self.log('train/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log('train/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log('train/text_loss', text_loss)\n",
    "\n",
    "        self.log('train/fine_grained_loss', fine_grained_loss)\n",
    "        self.log('train/super_loss', super_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='validation')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = torch.mean(torch.Tensor([output[f'{fine_grained_label}_loss'] for fine_grained_label in self.fine_grained_labels]))\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'])\n",
    "        self.log(f'val/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log(f'val/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log(f'val/text_loss', text_loss)\n",
    "\n",
    "\n",
    "        # TODO include this logic if needed\n",
    "        if self.fine_grained_labels and self.compute_fine_grained_metrics:\n",
    "            self.log(f'val/fine_grained_loss', fine_grained_loss)\n",
    "            self.log(f'val/super_loss', super_loss)\n",
    "\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_accuracy', output[f'{fine_grained_label}_accuracy'])\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_auroc', output[f'{fine_grained_label}_auroc'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_precision', output[f'{fine_grained_label}_precision'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_recall', output[f'{fine_grained_label}_recall'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_f1', output[f'{fine_grained_label}_f1'])\n",
    "\n",
    "            self.log(f'val/super_loss', output['super_loss'])\n",
    "            self.log(f'val/super_accuracy', output['super_accuracy'])\n",
    "            self.log(f'val/super_auroc', output['super_auroc'])\n",
    "\n",
    "        self.validation_step_outputs.append(total_loss)\n",
    "        return total_loss\n",
    "\n",
    "    # def on_train_epoch_end(self, validation_step_outputs):\n",
    "    def on_train_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "    # def on_validation_epoch_end(self, validation_step_outputs):\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    # def test_epoch_end(self, validation_step_outputs):\n",
    "    #     self.acc.reset()\n",
    "    #     self.auroc.reset()\n",
    "    #     self.precision_score.reset()\n",
    "    #     self.recall.reset()\n",
    "    #     self.f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
    "            ]\n",
    "        # print(\"what are params \", param_dicts)\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def create_model(args, fine_grained_labels):\n",
    "    compute_fine_grained_metrics = True\n",
    "    model = CLIPClassifier(args=args, fine_grained_labels=fine_grained_labels, compute_fine_grained_metrics = compute_fine_grained_metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23be74d9-efa7-41dc-a23a-c1af110f277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup params\n",
    "clip_model = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# default_param = {\n",
    "#     \"use_pretrained_map\": False,\n",
    "#     \"num_mapping_layers\": 1,\n",
    "#     \"map_dim\": 768,\n",
    "#     \"fusion\": \"align\",\n",
    "#     \"num_pre_output_layers\": 1,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"weight_decay\": 1e4,\n",
    "#     \"weight_image_loss\": 1.0,\n",
    "#     \"weight_text_loss\": 1.0,\n",
    "#     \"weight_fine_grained_loss\": 1.0,\n",
    "#     \"weight_super_loss\": 1.0,\n",
    "#     \"fine_grained_labels\": [],\n",
    "#     \"clip_pretrained_model\": clip_model,\n",
    "#     \"drop_probs\": 0.1,\n",
    "#     \"freeze_image_encoder\": True,\n",
    "#     \"freeze_text_encoder\": True\n",
    "# }\n",
    "\n",
    "# default_param = {\n",
    "#     \"use_pretrained_map\": False,\n",
    "#     \"num_mapping_layers\": 1,\n",
    "#     \"map_dim\": 768,\n",
    "#     \"fusion\": \"align\",\n",
    "#     \"num_pre_output_layers\": 1,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"weight_decay\": 1e4,\n",
    "#     \"weight_image_loss\": 1.0,\n",
    "#     \"weight_text_loss\": 1.0,\n",
    "#     \"weight_fine_grained_loss\": 1.0,\n",
    "#     \"weight_super_loss\": 1.0,\n",
    "#     \"fine_grained_labels\": [],\n",
    "#     \"clip_pretrained_model\": clip_model,\n",
    "#     \"drop_probs\": 0.1,\n",
    "#     \"freeze_image_encoder\": True,\n",
    "#     \"freeze_text_encoder\": True\n",
    "# }\n",
    "\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0,\n",
    "    \"weight_image_loss\": 0,\n",
    "    \"weight_text_loss\": 0,\n",
    "    \"weight_fine_grained_loss\": 0,\n",
    "    \"weight_super_loss\": 0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "\n",
    "model = create_model(default_param, fine_grained_labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b40b823-a22b-4803-aea3-7629ebb13664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (acc): BinaryAccuracy()\n",
       "  (auroc): BinaryAUROC()\n",
       "  (image_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_image): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_text): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (cross_entropy_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cddf080-0c0d-45da-86fc-499d7223928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# monitor=\"val/auroc\"\n",
    "# project=\"meme-v2\"\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename=project,  monitor=monitor, \n",
    "#                                       mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a80e2a0-5d6c-48ad-b65b-ba9bc8cbf57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# seed_everything(42)\n",
    "model = create_model(default_param, fine_grained_labels=[])\n",
    "\n",
    "\n",
    "# #TODO add GPU later\n",
    "# gpus=args.gpus\n",
    "\n",
    "max_steps = -1\n",
    "gradient_clip_val = 0.1\n",
    "log_every_n_steps = 5\n",
    "max_epochs = 20\n",
    "val_check_interval = 1.0\n",
    "limit_train_batches = 1.0\n",
    "limit_val_batches = 1.0\n",
    "\n",
    "monitor=\"val/auroc\"\n",
    "project=\"meme-v2\"\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename='checkpointFile',  \n",
    "                                      monitor=monitor, mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)\n",
    "\n",
    "# accelerator=\"cpu\", devices=2\n",
    "# accelerator=\"gpu\", devices=1\n",
    "\n",
    "# mode data and model to device (GPU)\n",
    "\n",
    "trainer = Trainer(max_epochs=max_epochs, max_steps=max_steps, gradient_clip_val=gradient_clip_val, \n",
    "        log_every_n_steps=log_every_n_steps, val_check_interval=val_check_interval, accelerator=\"gpu\", devices=1,\n",
    "        strategy=\"auto\", callbacks=[checkpoint_callback],\n",
    "        limit_train_batches=limit_train_batches, limit_val_batches=limit_val_batches,\n",
    "        deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7eebd455-0b16-4c0a-a44a-dbca8e9566f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name               | Type                  | Params\n",
      "--------------------------------------------------------------\n",
      "0  | acc                | BinaryAccuracy        | 0     \n",
      "1  | auroc              | BinaryAUROC           | 0     \n",
      "2  | image_encoder      | CLIPVisionTransformer | 87.5 M\n",
      "3  | text_encoder       | CLIPTextTransformer   | 63.2 M\n",
      "4  | image_map          | Sequential            | 787 K \n",
      "5  | text_map           | Sequential            | 656 K \n",
      "6  | pre_output         | Sequential            | 590 K \n",
      "7  | output             | Linear                | 769   \n",
      "8  | output_image       | Linear                | 769   \n",
      "9  | output_text        | Linear                | 769   \n",
      "10 | cross_entropy_loss | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "150 M     Non-trainable params\n",
      "152 M     Total params\n",
      "610.633   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 33/33 [02:58<00:00,  0.19it/s, v_num=30]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 33: 'val/auroc' reached 0.67213 (best 0.67213), saving model to 'checkpoints/checkpointFile-v41.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 33/33 [02:53<00:00,  0.19it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 66: 'val/auroc' reached 0.68065 (best 0.68065), saving model to 'checkpoints/checkpointFile-v42.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 33/33 [02:45<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 99: 'val/auroc' reached 0.69278 (best 0.69278), saving model to 'checkpoints/checkpointFile-v43.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 33/33 [02:42<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 132: 'val/auroc' reached 0.70087 (best 0.70087), saving model to 'checkpoints/checkpointFile-v41.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 33/33 [02:45<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 165: 'val/auroc' reached 0.69835 (best 0.70087), saving model to 'checkpoints/checkpointFile-v42.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 33/33 [02:45<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 198: 'val/auroc' reached 0.71976 (best 0.71976), saving model to 'checkpoints/checkpointFile-v43.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 33/33 [02:42<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 231: 'val/auroc' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 33/33 [02:45<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 264: 'val/auroc' reached 0.69911 (best 0.71976), saving model to 'checkpoints/checkpointFile-v42.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 33/33 [02:42<00:00,  0.20it/s, v_num=30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 297: 'val/auroc' reached 0.70452 (best 0.71976), saving model to 'checkpoints/checkpointFile-v42.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/33 [00:00<?, ?it/s, v_num=30]         "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dcc65d5-3f56-4a7c-9dcf-7c4a4bcf0984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndefault_param = {\\n    \"use_pretrained_map\": False,\\n    \"num_mapping_layers\": 1,\\n    \"map_dim\": 768,\\n    \"fusion\": \"align\",\\n    \"num_pre_output_layers\": 1,\\n    \"lr\": 1e-2,\\n    \"weight_decay\": 0,\\n    \"weight_image_loss\": 0,\\n    \"weight_text_loss\": 0,\\n    \"weight_fine_grained_loss\": 0,\\n    \"weight_super_loss\": 0,\\n    \"fine_grained_labels\": [],\\n    \"clip_pretrained_model\": clip_model,\\n    \"drop_probs\": 0.1,\\n    \"freeze_image_encoder\": True,\\n    \"freeze_text_encoder\": True\\n}\\n\\nbest is 0.75155 v_22\\n------------------------------------------\\n\\n\\ndefault_param = {\\n    \"use_pretrained_map\": False,\\n    \"num_mapping_layers\": 1,\\n    \"map_dim\": 768,\\n    \"fusion\": \"align\",\\n    \"num_pre_output_layers\": 1,\\n    \"lr\": 1e-4,\\n    \"weight_decay\": 0,\\n    \"weight_image_loss\": 0,\\n    \"weight_text_loss\": 0,\\n    \"weight_fine_grained_loss\": 0,\\n    \"weight_super_loss\": 0,\\n    \"fine_grained_labels\": [],\\n    \"clip_pretrained_model\": clip_model,\\n    \"drop_probs\": 0.1,\\n    \"freeze_image_encoder\": True,\\n    \"freeze_text_encoder\": True\\n}\\n\\n\\nbest is 0.69688  at v31\\n\\n\\n------------------------------------\\ndefault_param = {\\n    \"use_pretrained_map\": False,\\n    \"num_mapping_layers\": 1,\\n    \"map_dim\": 768,\\n    \"fusion\": \"align\",\\n    \"num_pre_output_layers\": 1,\\n    \"lr\": 1e-2,\\n    \"weight_decay\": 0,\\n    \"weight_image_loss\": 0,\\n    \"weight_text_loss\": 0,\\n    \"weight_fine_grained_loss\": 0,\\n    \"weight_super_loss\": 0,\\n    \"fine_grained_labels\": [],\\n    \"clip_pretrained_model\": clip_model,\\n    \"drop_probs\": 0.4,\\n    \"freeze_image_encoder\": True,\\n    \"freeze_text_encoder\": True\\n}\\n\\nbest is 0.73214 v33\\n\\n '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default_param = {\n",
    "#     \"use_pretrained_map\": False,\n",
    "#     \"num_mapping_layers\": 1,\n",
    "#     \"map_dim\": 768,\n",
    "#     \"fusion\": \"align\",\n",
    "#     \"num_pre_output_layers\": 1,\n",
    "#     \"lr\": 1e-2,\n",
    "#     \"weight_decay\": 1e-2,\n",
    "#     \"weight_image_loss\": 0,\n",
    "#     \"weight_text_loss\": 0,\n",
    "#     \"weight_fine_grained_loss\": 0,\n",
    "#     \"weight_super_loss\": 0,\n",
    "#     \"fine_grained_labels\": [],\n",
    "#     \"clip_pretrained_model\": clip_model,\n",
    "#     \"drop_probs\": 0.1,\n",
    "#     \"freeze_image_encoder\": True,\n",
    "#     \"freeze_text_encoder\": True\n",
    "# }\n",
    "# best is 0.7165 v_20\n",
    "\n",
    "\"\"\" \n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0,\n",
    "    \"weight_image_loss\": 0,\n",
    "    \"weight_text_loss\": 0,\n",
    "    \"weight_fine_grained_loss\": 0,\n",
    "    \"weight_super_loss\": 0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "best is 0.75155 v_22\n",
    "------------------------------------------\n",
    "\n",
    "\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"weight_image_loss\": 0,\n",
    "    \"weight_text_loss\": 0,\n",
    "    \"weight_fine_grained_loss\": 0,\n",
    "    \"weight_super_loss\": 0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "\n",
    "best is 0.69688  at v31\n",
    "\n",
    "\n",
    "------------------------------------\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0,\n",
    "    \"weight_image_loss\": 0,\n",
    "    \"weight_text_loss\": 0,\n",
    "    \"weight_fine_grained_loss\": 0,\n",
    "    \"weight_super_loss\": 0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.4,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "best is 0.73214 v33\n",
    "\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "809b45cd-3c24-4cf6-83c6-556e0f7d50e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3603761968.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [{\"ACMImageMulticlassExperimental0\":null,\"AdultImage\":null,\"AoaiCompletionAbuseList\":false,\"AoaiPromptAbuseList\":false,\"AoaiPromptJailbreakAbuseList\":false,\"BingBlockList\":false,\"BingCreativeAndSensitiveTopicsPattern\":false,\"BingCreativePattern\":false,\"BingMaskList\":false,\"BingSensitiveTopicsPattern\":false,\"BingSoftBlockList\":false,\"CodeVulnerability\":[{\"EntityType\":\"sql-injection\",\"Val\":false,\"Probability\":0.1},{\"EntityType\":\"hardcoded-credentials\",\"Val\":false,\"Probability\":0.04}],\"CombinedJailbreak\":true,\"Conv_MultiSeverity_HateSpeech\":null,\"Conv_MultiSeverity_HateSpeechScore\":null,\"Conv_MultiSeverity_SelfHarm\":null,\"Conv_MultiSeverity_SelfHarmScore\":null,\"Conv_MultiSeverity_Sexual\":null,\"Conv_MultiSeverity_SexualScore\":null,\"Conv_MultiSeverity_Violence\":null,\"Conv_MultiSeverity_ViolenceScore\":null,\"CreativeTextCopyright\":null,\"CustomerBlocklist\":null,\"DisinfoTopics\":false,\"DisinfoTopicsAbortionPattern\":false,\"DisinfoTopicsBLMPattern\":false,\"DisinfoTopicsCovidPattern\":false,\"DisinfoTopicsPattern\":false,\"DisinfoTopicsUSElectionsPattern\":false,\"DisinfoTopicsUkrainePattern\":false,\"GoreImage\":null,\"HateSpeech\":true,\"HateSpeechNeural\":0.97,\"HateSpeechPattern\":false,\"IPCodeCitations\":null,\"IPNLCitations\":null,\"ImageProvenance\":null,\"Img_FastMultiSeverity_HateSpeech\":null,\"Img_FastMultiSeverity_HateSpeechScore\":null,\"Img_FastMultiSeverity_SelfHarm\":null,\"Img_FastMultiSeverity_SelfHarmScore\":null,\"Img_FastMultiSeverity_Sexual\":null,\"Img_FastMultiSeverity_SexualScore\":null,\"Img_FastMultiSeverity_Violence\":null,\"Img_FastMultiSeverity_ViolenceScore\":null,\"Img_MultiSeverity_HateSpeech\":null,\"Img_MultiSeverity_HateSpeechScore\":null,\"Img_MultiSeverity_SelfHarm\":null,\"Img_MultiSeverity_SelfHarmScore\":null,\"Img_MultiSeverity_Sexual\":null,\"Img_MultiSeverity_SexualScore\":null,\"Img_MultiSeverity_Violence\":null,\"Img_MultiSeverity_ViolenceScore\":null,\"Inappropriate\":0.94067967,\"InterleaveTxtImg_MultiSeverity_HateSpeech\":null,\"InterleaveTxtImg_MultiSeverity_HateSpeechScore\":null,\"InterleaveTxtImg_MultiSeverity_SelfHarm\":null,\"InterleaveTxtImg_MultiSeverity_SelfHarmScore\":null,\"InterleaveTxtImg_MultiSeverity_Sexual\":null,\"InterleaveTxtImg_MultiSeverity_SexualScore\":null,\"InterleaveTxtImg_MultiSeverity_Violence\":null,\"InterleaveTxtImg_MultiSeverity_ViolenceScore\":null,\"Jailbreak\":{\"Val\":true,\"Probability\":0.99966633},\"JailbreakBlockList\":false,\"M365CompletionBlockList\":false,\"M365PromptBlockList\":false,\"Malware\":false,\"MalwareNeural\":0.01,\"MultiSeverity_HateSpeech\":{\"0\":1,\"1\":0.99,\"2\":0.97,\"3\":0.88,\"4\":0.765,\"5\":0.4,\"6\":0.2,\"7\":0.1},\"MultiSeverity_HateSpeechDangerous\":0.1,\"MultiSeverity_HateSpeechExplicit\":0.2,\"MultiSeverity_HateSpeechLow\":0.99,\"MultiSeverity_HateSpeechMature\":0.4,\"MultiSeverity_HateSpeechNotable\":0.97,\"MultiSeverity_HateSpeechOvert\":0.765,\"MultiSeverity_HateSpeechQuestionable\":0.88,\"MultiSeverity_HateSpeechScore\":7,\"MultiSeverity_SelfHarm\":{\"0\":1,\"1\":0.9881604,\"2\":0.9881714,\"3\":0.97852737,\"4\":0.032060772,\"5\":0.017656874,\"6\":0.0029615331,\"7\":0.000039016853},\"MultiSeverity_SelfHarmDangerous\":0.000039016853,\"MultiSeverity_SelfHarmExplicit\":0.0029615331,\"MultiSeverity_SelfHarmLow\":0.9881604,\"MultiSeverity_SelfHarmMature\":0.017656874,\"MultiSeverity_SelfHarmNotable\":0.9881714,\"MultiSeverity_SelfHarmOvert\":0.032060772,\"MultiSeverity_SelfHarmQuestionable\":0.97852737,\"MultiSeverity_SelfHarmScore\":6,\"MultiSeverity_Sexual\":{\"0\":1,\"1\":0.9881604,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"MultiSeverity_SexualDangerous\":0.01,\"MultiSeverity_SexualExplicit\":0.02,\"MultiSeverity_SexualLow\":0.9881604,\"MultiSeverity_SexualMature\":0.04,\"MultiSeverity_SexualNotable\":0.01,\"MultiSeverity_SexualOvert\":0.0765,\"MultiSeverity_SexualQuestionable\":0.088,\"MultiSeverity_SexualScore\":0,\"MultiSeverity_Violence\":{\"0\":1,\"1\":0.99,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"MultiSeverity_ViolenceDangerous\":0.01,\"MultiSeverity_ViolenceExplicit\":0.02,\"MultiSeverity_ViolenceLow\":0.99,\"MultiSeverity_ViolenceMature\":0.04,\"MultiSeverity_ViolenceNotable\":0.01,\"MultiSeverity_ViolenceOvert\":0.0765,\"MultiSeverity_ViolenceQuestionable\":0.088,\"MultiSeverity_ViolenceScore\":4,\"Offensive\":0.994259,\"OfficeGuardlistLowPrecision\":{\"Profanity\":\"HiPriority\",\"Weapons\":\"LoPriority\"},\"PMNLCitations\":null,\"Profanity\":false,\"ProfanityPattern\":false,\"PromBlockList\":false,\"PromptInjection_CrossDomain\":{\"Val\":true,\"Probability\":0.99966633},\"PromptInjection_Jailbreak\":{\"Val\":true,\"Probability\":0.99966633},\"PromptPromBlockList\":false,\"PublicFigures\":false,\"PublicFiguresPattern\":false,\"RacyImage\":null,\"SelfHarm\":true,\"SelfHarmNeural\":0.9881714,\"SelfHarmPattern\":false,\"SensitiveTopics\":[\"Corona\"],\"SensitiveTopicsProbability\":0.90274304,\"Sexual\":false,\"SexualNeural\":0.01,\"SexualPattern\":false,\"StoppingLogic\":true,\"SuicideHelp\":0.06913209,\"TextCopyright\":null,\"TextImg_MultiSeverity_HateSpeech\":{\"0\":0.02,\"2\":0.01,\"4\":0.96,\"6\":0.01},\"TextImg_MultiSeverity_HateSpeechScore\":3.96,\"TextToImageLowPrecision\":false,\"TextToImageLowPrecisionPattern\":false,\"Txt2CodeLowAUC\":false,\"Txt2CodeLowAUCObscenePattern\":false,\"Txt2CodeLowAUCSensitivePattern\":false,\"Txt2CodeLowAUCViolencePattern\":false,\"Txt_FastMultiSeverity_HateSpeech\":{\"0\":1,\"1\":0.99,\"2\":0.97,\"3\":0.88,\"4\":0.765,\"5\":0.4,\"6\":0.2,\"7\":0.1},\"Txt_FastMultiSeverity_HateSpeechDangerous\":0.1,\"Txt_FastMultiSeverity_HateSpeechExplicit\":0.2,\"Txt_FastMultiSeverity_HateSpeechLow\":0.99,\"Txt_FastMultiSeverity_HateSpeechMature\":0.4,\"Txt_FastMultiSeverity_HateSpeechNotable\":0.97,\"Txt_FastMultiSeverity_HateSpeechOvert\":0.765,\"Txt_FastMultiSeverity_HateSpeechQuestionable\":0.88,\"Txt_FastMultiSeverity_HateSpeechScore\":7,\"Txt_FastMultiSeverity_SelfHarm\":{\"0\":1,\"1\":0.9881604,\"2\":0.9881714,\"3\":0.97852737,\"4\":0.032060772,\"5\":0.017656874,\"6\":0.0029615331,\"7\":0.000039016853},\"Txt_FastMultiSeverity_SelfHarmDangerous\":0.000039016853,\"Txt_FastMultiSeverity_SelfHarmExplicit\":0.0029615331,\"Txt_FastMultiSeverity_SelfHarmLow\":0.9881604,\"Txt_FastMultiSeverity_SelfHarmMature\":0.017656874,\"Txt_FastMultiSeverity_SelfHarmNotable\":0.9881714,\"Txt_FastMultiSeverity_SelfHarmOvert\":0.032060772,\"Txt_FastMultiSeverity_SelfHarmQuestionable\":0.97852737,\"Txt_FastMultiSeverity_SelfHarmScore\":6,\"Txt_FastMultiSeverity_Sexual\":{\"0\":1,\"1\":0.9881604,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"Txt_FastMultiSeverity_SexualDangerous\":0.01,\"Txt_FastMultiSeverity_SexualExplicit\":0.02,\"Txt_FastMultiSeverity_SexualLow\":0.9881604,\"Txt_FastMultiSeverity_SexualMature\":0.04,\"Txt_FastMultiSeverity_SexualNotable\":0.01,\"Txt_FastMultiSeverity_SexualOvert\":0.0765,\"Txt_FastMultiSeverity_SexualQuestionable\":0.088,\"Txt_FastMultiSeverity_SexualScore\":0,\"Txt_FastMultiSeverity_Violence\":{\"0\":1,\"1\":0.99,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"Txt_FastMultiSeverity_ViolenceDangerous\":0.01,\"Txt_FastMultiSeverity_ViolenceExplicit\":0.02,\"Txt_FastMultiSeverity_ViolenceLow\":0.99,\"Txt_FastMultiSeverity_ViolenceMature\":0.04,\"Txt_FastMultiSeverity_ViolenceNotable\":0.01,\"Txt_FastMultiSeverity_ViolenceOvert\":0.0765,\"Txt_FastMultiSeverity_ViolenceQuestionable\":0.088,\"Txt_FastMultiSev\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "[{\"ACMImageMulticlassExperimental0\":null,\"AdultImage\":null,\"AoaiCompletionAbuseList\":false,\"AoaiPromptAbuseList\":false,\"AoaiPromptJailbreakAbuseList\":false,\"BingBlockList\":false,\"BingCreativeAndSensitiveTopicsPattern\":false,\"BingCreativePattern\":false,\"BingMaskList\":false,\"BingSensitiveTopicsPattern\":false,\"BingSoftBlockList\":false,\"CodeVulnerability\":[{\"EntityType\":\"sql-injection\",\"Val\":false,\"Probability\":0.1},{\"EntityType\":\"hardcoded-credentials\",\"Val\":false,\"Probability\":0.04}],\"CombinedJailbreak\":true,\"Conv_MultiSeverity_HateSpeech\":null,\"Conv_MultiSeverity_HateSpeechScore\":null,\"Conv_MultiSeverity_SelfHarm\":null,\"Conv_MultiSeverity_SelfHarmScore\":null,\"Conv_MultiSeverity_Sexual\":null,\"Conv_MultiSeverity_SexualScore\":null,\"Conv_MultiSeverity_Violence\":null,\"Conv_MultiSeverity_ViolenceScore\":null,\"CreativeTextCopyright\":null,\"CustomerBlocklist\":null,\"DisinfoTopics\":false,\"DisinfoTopicsAbortionPattern\":false,\"DisinfoTopicsBLMPattern\":false,\"DisinfoTopicsCovidPattern\":false,\"DisinfoTopicsPattern\":false,\"DisinfoTopicsUSElectionsPattern\":false,\"DisinfoTopicsUkrainePattern\":false,\"GoreImage\":null,\"HateSpeech\":true,\"HateSpeechNeural\":0.97,\"HateSpeechPattern\":false,\"IPCodeCitations\":null,\"IPNLCitations\":null,\"ImageProvenance\":null,\"Img_FastMultiSeverity_HateSpeech\":null,\"Img_FastMultiSeverity_HateSpeechScore\":null,\"Img_FastMultiSeverity_SelfHarm\":null,\"Img_FastMultiSeverity_SelfHarmScore\":null,\"Img_FastMultiSeverity_Sexual\":null,\"Img_FastMultiSeverity_SexualScore\":null,\"Img_FastMultiSeverity_Violence\":null,\"Img_FastMultiSeverity_ViolenceScore\":null,\"Img_MultiSeverity_HateSpeech\":null,\"Img_MultiSeverity_HateSpeechScore\":null,\"Img_MultiSeverity_SelfHarm\":null,\"Img_MultiSeverity_SelfHarmScore\":null,\"Img_MultiSeverity_Sexual\":null,\"Img_MultiSeverity_SexualScore\":null,\"Img_MultiSeverity_Violence\":null,\"Img_MultiSeverity_ViolenceScore\":null,\"Inappropriate\":0.94067967,\"InterleaveTxtImg_MultiSeverity_HateSpeech\":null,\"InterleaveTxtImg_MultiSeverity_HateSpeechScore\":null,\"InterleaveTxtImg_MultiSeverity_SelfHarm\":null,\"InterleaveTxtImg_MultiSeverity_SelfHarmScore\":null,\"InterleaveTxtImg_MultiSeverity_Sexual\":null,\"InterleaveTxtImg_MultiSeverity_SexualScore\":null,\"InterleaveTxtImg_MultiSeverity_Violence\":null,\"InterleaveTxtImg_MultiSeverity_ViolenceScore\":null,\"Jailbreak\":{\"Val\":true,\"Probability\":0.99966633},\"JailbreakBlockList\":false,\"M365CompletionBlockList\":false,\"M365PromptBlockList\":false,\"Malware\":false,\"MalwareNeural\":0.01,\"MultiSeverity_HateSpeech\":{\"0\":1,\"1\":0.99,\"2\":0.97,\"3\":0.88,\"4\":0.765,\"5\":0.4,\"6\":0.2,\"7\":0.1},\"MultiSeverity_HateSpeechDangerous\":0.1,\"MultiSeverity_HateSpeechExplicit\":0.2,\"MultiSeverity_HateSpeechLow\":0.99,\"MultiSeverity_HateSpeechMature\":0.4,\"MultiSeverity_HateSpeechNotable\":0.97,\"MultiSeverity_HateSpeechOvert\":0.765,\"MultiSeverity_HateSpeechQuestionable\":0.88,\"MultiSeverity_HateSpeechScore\":7,\"MultiSeverity_SelfHarm\":{\"0\":1,\"1\":0.9881604,\"2\":0.9881714,\"3\":0.97852737,\"4\":0.032060772,\"5\":0.017656874,\"6\":0.0029615331,\"7\":0.000039016853},\"MultiSeverity_SelfHarmDangerous\":0.000039016853,\"MultiSeverity_SelfHarmExplicit\":0.0029615331,\"MultiSeverity_SelfHarmLow\":0.9881604,\"MultiSeverity_SelfHarmMature\":0.017656874,\"MultiSeverity_SelfHarmNotable\":0.9881714,\"MultiSeverity_SelfHarmOvert\":0.032060772,\"MultiSeverity_SelfHarmQuestionable\":0.97852737,\"MultiSeverity_SelfHarmScore\":6,\"MultiSeverity_Sexual\":{\"0\":1,\"1\":0.9881604,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"MultiSeverity_SexualDangerous\":0.01,\"MultiSeverity_SexualExplicit\":0.02,\"MultiSeverity_SexualLow\":0.9881604,\"MultiSeverity_SexualMature\":0.04,\"MultiSeverity_SexualNotable\":0.01,\"MultiSeverity_SexualOvert\":0.0765,\"MultiSeverity_SexualQuestionable\":0.088,\"MultiSeverity_SexualScore\":0,\"MultiSeverity_Violence\":{\"0\":1,\"1\":0.99,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"MultiSeverity_ViolenceDangerous\":0.01,\"MultiSeverity_ViolenceExplicit\":0.02,\"MultiSeverity_ViolenceLow\":0.99,\"MultiSeverity_ViolenceMature\":0.04,\"MultiSeverity_ViolenceNotable\":0.01,\"MultiSeverity_ViolenceOvert\":0.0765,\"MultiSeverity_ViolenceQuestionable\":0.088,\"MultiSeverity_ViolenceScore\":4,\"Offensive\":0.994259,\"OfficeGuardlistLowPrecision\":{\"Profanity\":\"HiPriority\",\"Weapons\":\"LoPriority\"},\"PMNLCitations\":null,\"Profanity\":false,\"ProfanityPattern\":false,\"PromBlockList\":false,\"PromptInjection_CrossDomain\":{\"Val\":true,\"Probability\":0.99966633},\"PromptInjection_Jailbreak\":{\"Val\":true,\"Probability\":0.99966633},\"PromptPromBlockList\":false,\"PublicFigures\":false,\"PublicFiguresPattern\":false,\"RacyImage\":null,\"SelfHarm\":true,\"SelfHarmNeural\":0.9881714,\"SelfHarmPattern\":false,\"SensitiveTopics\":[\"Corona\"],\"SensitiveTopicsProbability\":0.90274304,\"Sexual\":false,\"SexualNeural\":0.01,\"SexualPattern\":false,\"StoppingLogic\":true,\"SuicideHelp\":0.06913209,\"TextCopyright\":null,\"TextImg_MultiSeverity_HateSpeech\":{\"0\":0.02,\"2\":0.01,\"4\":0.96,\"6\":0.01},\"TextImg_MultiSeverity_HateSpeechScore\":3.96,\"TextToImageLowPrecision\":false,\"TextToImageLowPrecisionPattern\":false,\"Txt2CodeLowAUC\":false,\"Txt2CodeLowAUCObscenePattern\":false,\"Txt2CodeLowAUCSensitivePattern\":false,\"Txt2CodeLowAUCViolencePattern\":false,\"Txt_FastMultiSeverity_HateSpeech\":{\"0\":1,\"1\":0.99,\"2\":0.97,\"3\":0.88,\"4\":0.765,\"5\":0.4,\"6\":0.2,\"7\":0.1},\"Txt_FastMultiSeverity_HateSpeechDangerous\":0.1,\"Txt_FastMultiSeverity_HateSpeechExplicit\":0.2,\"Txt_FastMultiSeverity_HateSpeechLow\":0.99,\"Txt_FastMultiSeverity_HateSpeechMature\":0.4,\"Txt_FastMultiSeverity_HateSpeechNotable\":0.97,\"Txt_FastMultiSeverity_HateSpeechOvert\":0.765,\"Txt_FastMultiSeverity_HateSpeechQuestionable\":0.88,\"Txt_FastMultiSeverity_HateSpeechScore\":7,\"Txt_FastMultiSeverity_SelfHarm\":{\"0\":1,\"1\":0.9881604,\"2\":0.9881714,\"3\":0.97852737,\"4\":0.032060772,\"5\":0.017656874,\"6\":0.0029615331,\"7\":0.000039016853},\"Txt_FastMultiSeverity_SelfHarmDangerous\":0.000039016853,\"Txt_FastMultiSeverity_SelfHarmExplicit\":0.0029615331,\"Txt_FastMultiSeverity_SelfHarmLow\":0.9881604,\"Txt_FastMultiSeverity_SelfHarmMature\":0.017656874,\"Txt_FastMultiSeverity_SelfHarmNotable\":0.9881714,\"Txt_FastMultiSeverity_SelfHarmOvert\":0.032060772,\"Txt_FastMultiSeverity_SelfHarmQuestionable\":0.97852737,\"Txt_FastMultiSeverity_SelfHarmScore\":6,\"Txt_FastMultiSeverity_Sexual\":{\"0\":1,\"1\":0.9881604,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"Txt_FastMultiSeverity_SexualDangerous\":0.01,\"Txt_FastMultiSeverity_SexualExplicit\":0.02,\"Txt_FastMultiSeverity_SexualLow\":0.9881604,\"Txt_FastMultiSeverity_SexualMature\":0.04,\"Txt_FastMultiSeverity_SexualNotable\":0.01,\"Txt_FastMultiSeverity_SexualOvert\":0.0765,\"Txt_FastMultiSeverity_SexualQuestionable\":0.088,\"Txt_FastMultiSeverity_SexualScore\":0,\"Txt_FastMultiSeverity_Violence\":{\"0\":1,\"1\":0.99,\"2\":0.01,\"3\":0.088,\"4\":0.0765,\"5\":0.04,\"6\":0.02,\"7\":0.01},\"Txt_FastMultiSeverity_ViolenceDangerous\":0.01,\"Txt_FastMultiSeverity_ViolenceExplicit\":0.02,\"Txt_FastMultiSeverity_ViolenceLow\":0.99,\"Txt_FastMultiSeverity_ViolenceMature\":0.04,\"Txt_FastMultiSeverity_ViolenceNotable\":0.01,\"Txt_FastMultiSeverity_ViolenceOvert\":0.0765,\"Txt_FastMultiSeverity_ViolenceQuestionable\":0.088,\"Txt_FastMultiSev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505607b-c204-4047-8376-5a3f859c3762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b73bd44-dbb2-4f9a-98ec-7d55662cccd0",
   "metadata": {},
   "source": [
    "- Added 0 worker for dataloader and 1 device for trainer. The auroc is 0.448 and it is unchanged through different epochs.\n",
    "- <the issue could be that configure_optimizer is not taking the right parameters and updating the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2b165-662a-428d-8e24-0d397cb94de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip_prefix_caption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

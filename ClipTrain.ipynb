{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "c8de9ac2-c7c5-4e43-86c7-cd160d761a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.1.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "b307e231-1fc1-43ea-9148-16ed4c82b2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "1a2ef165-798f-42b9-877b-028e8a01e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a466760c-7313-4fba-928d-2e1e6948df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import features\n",
    "import copy\n",
    "import math\n",
    "from sys import prefix\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import copy\n",
    "\n",
    "from transformers import CLIPModel, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "abfcec5d-d901-4486-92ca-26344a46c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import CLIPTokenizer, CLIPProcessor, AutoTokenizer\n",
    "\n",
    "\n",
    "class HatefulMemesDataset(Dataset):\n",
    "    def __init__(self, root_folder, image_folder, feature_file,fairfacefile,entityfile,split='train', labels='original', image_size=224):\n",
    "        super(HatefulMemesDataset, self).__init__()\n",
    "        self.root_folder = root_folder\n",
    "        self.image_folder = image_folder\n",
    "        self.split = split\n",
    "        self.labels = labels\n",
    "        self.image_size = image_size\n",
    "        self.info_file = f\"{self.split}.csv\"\n",
    "        self.features_df = pd.read_csv(f\"{self.split}_features.csv\")\n",
    "        self.fairface_df = pd.read_csv(f\"{fairfacefile}.csv\")\n",
    "        self.entity_df = pd.read_csv(f\"{entityfile}.csv\")\n",
    "        self.fairface_df=self.fairface_df[~self.fairface_df[\"face_name_align\"].str.contains(\"race\")]\n",
    "\n",
    "        print(\"data here: \", self.info_file)\n",
    "        self.df = pd.read_csv(self.info_file)\n",
    "        # float_cols = self.df.select_dtypes(float).columns\n",
    "        # self.df[float_cols] = self.df[float_cols].fillna(-1).astype('Int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feature_row=self.features_df[self.features_df[\"id\"]==row[\"id\"]]\n",
    "        entity_row = pd.DataFrame(self.entity_df[((self.entity_df[\"filename\"].str.split(\".\").str[0]).astype(int)) == row[\"id\"]])\n",
    "        values = entity_row[[\"description1\", \"description2\", \"description3\", \"description4\"]]\n",
    "        entity_string = ','.join([x for x in values.iloc[0] if x != '' and pd.notna(x)])\n",
    "        self.fairface_df[\"id\"]=self.fairface_df[\"face_name_align\"].str.split(\"/\").str[-1].str.split(\"_\").str[0]\n",
    "        fairface_rows = self.fairface_df[(self.fairface_df[\"id\"].astype(int) == row[\"id\"])]\n",
    "        fairface_string=''\n",
    "        combined_strings=[]\n",
    "        if len(fairface_rows)>0:\n",
    "            for ff_row in fairface_rows.itertuples():\n",
    "                combined_string = ', '.join([ff_row[2], ff_row[4], ff_row[5]])\n",
    "                combined_strings.append(combined_string)\n",
    "            fairface_string = '. '.join(combined_strings)\n",
    "        item = {}\n",
    "        image_fn = row['img'].split('/')[1]\n",
    "        item['image'] = Image.open(f\"{self.image_folder}/{image_fn}\").convert('RGB').resize((self.image_size, self.image_size))\n",
    "        item['text'] = row['text']+\". \"+fairface_string+\". \"+entity_string #remove fairface and entity strings to get baseline\n",
    "        item['label'] = row['label']\n",
    "        item['idx_meme'] = row['id']\n",
    "        additional_features = feature_row.iloc[0, 2:].values.astype('float32')  # Convert to float32 for PyTorch compatibility\n",
    "        item['additional_features'] = torch.tensor(additional_features)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "a7231e13-89db-4750-913b-3907df9bafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pre_trained_model = 'openai/clip-vit-base-patch32'\n",
    "        self.image_processor = CLIPProcessor.from_pretrained(pre_trained_model)\n",
    "        self.text_processor = CLIPTokenizer.from_pretrained(pre_trained_model)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = self.image_processor(images=[item['image'] for item in batch], return_tensors=\"pt\")['pixel_values']\n",
    "        text_output = self.text_processor([item['text'] for item in batch], padding=True, return_tensors=\"pt\", truncation=True,max_length=77) #change max length from 77 if using larger model\n",
    "        labels = torch.LongTensor([item['label'] for item in batch])\n",
    "        idx_memes = torch.LongTensor([item['idx_meme'] for item in batch])\n",
    "        additional_features=torch.stack([item['additional_features'] for item in batch])\n",
    "\n",
    "        batch_new = {}\n",
    "        batch_new['pixel_values'] = pixel_values,\n",
    "        batch_new['input_ids'] = text_output['input_ids']\n",
    "        batch_new['attention_mask'] = text_output['attention_mask']\n",
    "\n",
    "        batch_new['labels'] = labels\n",
    "        batch_new['idx_memes'] = idx_memes\n",
    "        batch_new['additional_features']=additional_features\n",
    "        return batch_new\n",
    "\n",
    "collator = CustomCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "0ab19eab-845b-4ad0-ae62-28023bbdfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores\n",
    "multilingual_tokenizer_path = 'none'\n",
    "fine_grained_labels = []\n",
    "compute_fine_grained_metrics = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2bece-bf7f-47d2-9db5-1e01e5b4c6ef",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "4c9a0dba-8b31-47f7-8ca5-baca8d773e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data here:  train.csv\n",
      "data here:  dev.csv\n"
     ]
    }
   ],
   "source": [
    "labels = \"original\"\n",
    "image_folder = 'data/img'\n",
    "image_size = 224\n",
    "\n",
    "dataset_train = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, split='train',\n",
    "            labels=labels, image_size=image_size)\n",
    "\n",
    "dataset_val = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, split='dev',\n",
    "            labels=labels, image_size=image_size)\n",
    "\n",
    "dataset_test = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, split='test',\n",
    "            labels=labels, image_size=image_size)\n",
    "\n",
    "# uncomment this if you wanna use pre-processing steps on validation dataset \n",
    "# dataset_train = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, feature_file='train_features.csv',fairfacefile='race_gender_age_all_10k',entityfile='entities_all_10k',split='train',\n",
    "#             labels=labels, image_size=image_size)\n",
    "\n",
    "# dataset_val = HatefulMemesDataset(root_folder='data/', image_folder=image_folder, feature_file='dev_features.csv',split='dev',\n",
    "#             labels=labels, image_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "a217b521-0ea1-4e16-abd1-4f68d7f66bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "6bc10f82-8260-4941-9ef5-a4d2f54402c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162D72021D0>, 'text': 'its their character not their color that matters. Black, Male, 30-39. Black, Male, 20-29. Facial hair,Forehead,Hair,Meter', 'label': 0, 'idx_meme': 42953, 'additional_features': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162E62DD710>, 'text': \"don't be afraid to love again everyone is not like your ex. Black, Male, 20-29. White, Female, 20-29. Bride,Photograph,Wedding,Wedding Dress\", 'label': 0, 'idx_meme': 23058, 'additional_features': tensor([0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 1.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162DCDA3210>, 'text': 'putting bows on your pet. . Chartreux,Korat,Mammal,Cat-like', 'label': 0, 'idx_meme': 13894, 'additional_features': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162DCD94D90>, 'text': 'i love everything and everybody! except for squirrels i hate squirrels. . Canidae,Rottweiler,Dog Breed,Carnivores', 'label': 0, 'idx_meme': 37408, 'additional_features': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162E1B70990>, 'text': 'everybody loves chocolate chip cookies, even hitler. . Heinrich Hoffmann,Hitler Was My Friend,Nazi Party,FÃ¼hrerbau', 'label': 0, 'idx_meme': 82403, 'additional_features': tensor([1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 3., 0., 0., 2.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162D5A5B2D0>, 'text': 'go sports! do the thing! win the points!. . Getty Images,Stock photography,Basketball', 'label': 0, 'idx_meme': 16952, 'additional_features': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162CB458A50>, 'text': \"fine you're right. now can we fucking drop it?. . Stock photography,Photograph,Image,Tiger\", 'label': 0, 'idx_meme': 76932, 'additional_features': tensor([2., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x1630399AA90>, 'text': 'tattoos are bad for your health i know 5 million people that had tattoos and they all died. . Tattoo,Female tattoos,Tattoo aftercare,Tattoo artist', 'label': 0, 'idx_meme': 70914, 'additional_features': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162DF440490>, 'text': 'how long can i run? till the chain tightens. . Nova Scotia Duck Tolling Retriever,Dog Breed,Canidae,Snout', 'label': 0, 'idx_meme': 2973, 'additional_features': tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])}\n",
      "{'image': <PIL.Image.Image image mode=RGB size=224x224 at 0x162D6DFB4D0>, 'text': 'what is he hiding? we need to see his tax returns! let me stop you right there hillary you deleted 30,000 emails, used bleach bit on hard drives, then destroyed phones with hammers you have no right to talk about people hiding anything truth uncensored. White, Female, 50-59. East Asian, Female, 20-29. Poster,Photo caption,Wallpaper,Text', 'label': 0, 'idx_meme': 58306, 'additional_features': tensor([0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 5., 0., 0., 0., 2.])}\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "64f34994-b05d-47a8-8cf2-1fa2fdd7635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, num_workers=0, collate_fn=collator)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=False, num_workers=0, collate_fn=collator)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=0, collate_fn=collator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "61651d53-9b65-4144-a878-7004a9a1ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_fine_grained_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "a082520f-e300-47ce-a50f-701e9fbd9351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x162dc019450>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb80d1c-b49c-4838-9df9-3e70b3a00fd2",
   "metadata": {},
   "source": [
    "# Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "2fc288bb-4726-4044-b021-0f27735ff57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, fine_grained_labels, compute_fine_grained_metrics):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.caption_mode = args.caption_mode\n",
    "        self.use_pretrained_map = args['use_pretrained_map']\n",
    "        self.num_mapping_layers = args['num_mapping_layers']\n",
    "        self.map_dim = args['map_dim']\n",
    "        self.fusion = args['fusion']\n",
    "        self.num_pre_output_layers = args['num_pre_output_layers']\n",
    "        self.lr = args['lr']\n",
    "        self.weight_decay = args['weight_decay']\n",
    "        self.weight_image_loss = args['weight_image_loss']\n",
    "        self.weight_text_loss = args['weight_text_loss']\n",
    "        self.weight_fine_grained_loss = args['weight_fine_grained_loss']\n",
    "        self.weight_super_loss = args['weight_super_loss']\n",
    "        self.fine_grained_labels = fine_grained_labels\n",
    "        self.compute_fine_grained_metrics = compute_fine_grained_metrics\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.auroc = torchmetrics.AUROC(task=\"binary\")\n",
    "        # self.precision_score = torchmetrics.Precision()\n",
    "        # self.recall = torchmetrics.Recall()\n",
    "        # self.f1 = torchmetrics.F1Score()\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        self.clip = CLIPModel.from_pretrained(args['clip_pretrained_model'])\n",
    "        self.image_encoder = copy.deepcopy(self.clip.vision_model)\n",
    "        self.text_encoder = copy.deepcopy(self.clip.text_model)\n",
    "        self.image_map = nn.Sequential(\n",
    "                copy.deepcopy(self.clip.visual_projection),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "                )\n",
    "        self.text_map = nn.Sequential(\n",
    "            copy.deepcopy(self.clip.text_projection),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.clip.projection_dim, self.map_dim)\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if args['fusion'] in ['align', 'align_shuffle']:\n",
    "            pre_output_input_dim = self.map_dim\n",
    "        elif args['fusion'] == 'concat':\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "        elif args['fusion'].startswith('cross'):\n",
    "            pre_output_input_dim = self.map_dim**2\n",
    "        elif args['fusion'] == 'align_concat':\n",
    "            pre_output_input_dim = self.map_dim*3\n",
    "        elif args['fusion'] == 'attention_m':\n",
    "            self.gen_query = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.gen_key = nn.Linear(self.map_dim, self.map_dim//4)\n",
    "            self.soft = nn.Softmax(dim=1)\n",
    "            pre_output_input_dim = self.map_dim*2\n",
    "\n",
    "        pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "        self.additional_feature_size=16 #set to zero if using baseline\n",
    "        pre_output_input_dim+=self.additional_feature_size\n",
    "\n",
    "\n",
    "        if self.num_pre_output_layers >= 1: # first pre-output layer\n",
    "            pre_output_layers.extend([nn.Linear(pre_output_input_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            output_input_dim = self.map_dim\n",
    "        for _ in range(1, self.num_pre_output_layers): # next pre-output layers\n",
    "            pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "\n",
    "        self.pre_output = nn.Sequential(*pre_output_layers)\n",
    "        self.output = nn.Linear(output_input_dim, 1)\n",
    "        self.output_image = nn.Linear(output_input_dim, 1)\n",
    "        self.output_text = nn.Linear(output_input_dim, 1)\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_image = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            pre_output_layers = [nn.Dropout(p=args['drop_probs'])]\n",
    "            for _ in range(self.num_pre_output_layers): # next pre-output layers\n",
    "                pre_output_layers.extend([nn.Linear(self.map_dim, self.map_dim), nn.ReLU(), nn.Dropout(p=args['drop_probs'])])\n",
    "            self.pre_output_text = nn.Sequential(*pre_output_layers)\n",
    "\n",
    "        if self.fine_grained_labels:\n",
    "            # if self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            self.output_pc1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_pc6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack1 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack2 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack3 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack4 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack5 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack6 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack7 = nn.Linear(output_input_dim, 1)\n",
    "            self.output_attack8 = nn.Linear(output_input_dim, 1)\n",
    "            self.outputs_fine_grained = [self.output_pc1, self.output_pc2, self.output_pc3, self.output_pc4, self.output_pc5, self.output_pc6,\n",
    "                self.output_attack1, self.output_attack2, self.output_attack3, self.output_attack4, self.output_attack5, self.output_attack6, self.output_attack7, self.output_attack8]\n",
    "            self.output_super = nn.Linear(15, 1)\n",
    "\n",
    "        self.cross_entropy_loss = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        if args['freeze_image_encoder']:\n",
    "            for _, p in self.image_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        if args['freeze_text_encoder']:\n",
    "            for _, p in self.text_encoder.named_parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "        del self.clip\n",
    "        # if self.caption_mode == 'replace_image':\n",
    "        #     del self.image_encoder, self.image_map\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1) # [batch_size, d]\n",
    "        text_features = F.normalize(text_features, p=2, dim=1) # [batch_size, d]\n",
    "\n",
    "        features = torch.mul(image_features, text_features)  # [batch_size, d]\n",
    "        features=torch.cat((features,batch[\"additional_features\"]),dim=1)\n",
    "        features = self.pre_output(features)\n",
    "        logits = self.output(features)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def common_step(self, batch, batch_idx, calling_function='validation'):\n",
    "        image_features = self.image_encoder(pixel_values=batch['pixel_values'][0]).pooler_output\n",
    "        image_features = self.image_map(image_features)\n",
    "\n",
    "        text_features = self.text_encoder(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).pooler_output\n",
    "        text_features = self.text_map(text_features)\n",
    "\n",
    "        image_features = F.normalize(image_features, p=2, dim=1)\n",
    "        text_features = F.normalize(text_features, p=2, dim=1)\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            features_pre_output = self.pre_output_image(image_features)\n",
    "            logits = self.output_image(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['image_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['image_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['image_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            features_pre_output = self.pre_output_text(text_features)\n",
    "            logits = self.output_text(features_pre_output).squeeze(dim=1) # [batch_size, 1]\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "            output['text_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['text_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['text_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        #features = torch.mul(image_features, text_features)\n",
    "\n",
    "        if self.fusion in ['align', 'align_shuffle']:\n",
    "            features = torch.mul(image_features, text_features)\n",
    "        elif self.fusion == 'concat':\n",
    "            features = torch.cat([image_features, text_features], dim=1)\n",
    "        elif self.fusion.startswith('cross'):\n",
    "            features = torch.bmm(image_features.unsqueeze(2), text_features.unsqueeze(1)) # [16, d, d]\n",
    "            if self.fusion == 'cross_nd':\n",
    "                mask = torch.eye(self.map_dim).repeat(features.shape[0], 1, 1).bool()\n",
    "                features[mask] = torch.zeros(features.shape[0]*self.map_dim, device=features.device)\n",
    "                del mask\n",
    "            features = features.reshape(features.shape[0], -1)  # [batch_size, d*d]\n",
    "        elif self.fusion == 'align_concat':\n",
    "                features = torch.cat([torch.mul(image_features, text_features), image_features, text_features], dim=1)  # [batch_size, 3*d]\n",
    "        elif self.fusion == 'attention_m':\n",
    "            q1 = F.relu(self.gen_query(image_features))\n",
    "            k1 = F.relu(self.gen_key(image_features))\n",
    "            q2 = F.relu(self.gen_query(text_features))\n",
    "            k2 = F.relu(self.gen_key(text_features))\n",
    "            score1 = torch.reshape(torch.bmm(q1.view(-1, 1, 256), k2.view(-1, 256, 1)), (-1, 1))\n",
    "            score2 = torch.reshape(torch.bmm(q2.view(-1, 1, 256), k1.view(-1, 256, 1)), (-1, 1))\n",
    "            wt_score1_score2_mat = torch.cat((score1, score2), 1)\n",
    "            wt_i1_i2 = self.soft(wt_score1_score2_mat.float()) #prob\n",
    "            prob_1 = wt_i1_i2[:,0]\n",
    "            prob_2 = wt_i1_i2[:,1]\n",
    "            wtd_i1 = image_features * prob_1[:, None]\n",
    "            wtd_i2 = text_features * prob_2[:, None]\n",
    "            features = torch.cat((wtd_i1,wtd_i2), 1) # [batch_size, 2*d]\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        \n",
    "        additional_features=batch['additional_features']\n",
    "        features=torch.cat((features, additional_features),dim=1) #remove cat if doing baseline\n",
    "        features_pre_output = self.pre_output(features)\n",
    "        logits = self.output(features_pre_output).squeeze(dim=1) # [batch_size, 1(or)n]\n",
    "        if self.fine_grained_labels and self.dataset in ['original', 'masked', 'inpainted']:\n",
    "            logits_for_super = [torch.relu(logits)]\n",
    "        preds_proxy = torch.sigmoid(logits)\n",
    "        preds = (preds_proxy >= 0.5).long()\n",
    "\n",
    "        output['loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "\n",
    "\n",
    "        \n",
    "        output['accuracy'] = self.acc(preds, batch['labels'])\n",
    "        output['auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "\n",
    "        if calling_function == 'training' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output['super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output['super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output['super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "\n",
    "        elif calling_function == 'validation' and self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            for fine_grained_label, output_fine_grained in zip(self.fine_grained_labels, self.outputs_fine_grained):\n",
    "                logits = output_fine_grained(features_pre_output).squeeze(dim=1)\n",
    "                logits_for_super.append(torch.relu(logits))\n",
    "                preds_proxy = torch.sigmoid(logits)\n",
    "                preds = (preds_proxy >= 0.5).long()\n",
    "                output[f'{fine_grained_label}_loss'] = self.cross_entropy_loss(logits, batch[fine_grained_label].float())\n",
    "                output[f'{fine_grained_label}_accuracy'] = self.acc(preds, batch[fine_grained_label])\n",
    "                output[f'{fine_grained_label}_auroc'] = self.auroc(preds_proxy, batch[fine_grained_label])\n",
    "                \n",
    "            logits_for_super = torch.stack(logits_for_super, dim=1) # [batch_size, 15]\n",
    "            logits = self.output_super(logits_for_super).squeeze(dim=1)\n",
    "            preds_proxy = torch.sigmoid(logits)\n",
    "            preds = (preds_proxy >= 0.5).long()\n",
    "            output[f'super_loss'] = self.cross_entropy_loss(logits, batch['labels'].float())\n",
    "            output[f'super_accuracy'] = self.acc(preds, batch['labels'])\n",
    "            output[f'super_auroc'] = self.auroc(preds_proxy, batch['labels'])\n",
    "\n",
    "        elif calling_function == 'visualisation-v1':\n",
    "            return image_features, text_features\n",
    "\n",
    "        elif calling_function == 'visualisation-v2':\n",
    "            return features\n",
    "\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='training')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = 0\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                fine_grained_loss += output[f'{fine_grained_label}_loss']\n",
    "            fine_grained_loss /= len(self.fine_grained_labels)\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log('train/total_loss', total_loss)\n",
    "        self.log('train/loss', output['loss'])\n",
    "        self.log('train/accuracy', output['accuracy'])\n",
    "        self.log('train/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log('train/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log('train/text_loss', text_loss)\n",
    "\n",
    "        self.log('train/fine_grained_loss', fine_grained_loss)\n",
    "        self.log('train/super_loss', super_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='validation')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = torch.mean(torch.Tensor([output[f'{fine_grained_label}_loss'] for fine_grained_label in self.fine_grained_labels]))\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'])\n",
    "        self.log(f'val/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log(f'val/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log(f'val/text_loss', text_loss)\n",
    "\n",
    "\n",
    "        # TODO include this logic if needed\n",
    "        if self.fine_grained_labels and self.compute_fine_grained_metrics:\n",
    "            self.log(f'val/fine_grained_loss', fine_grained_loss)\n",
    "            self.log(f'val/super_loss', super_loss)\n",
    "\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_accuracy', output[f'{fine_grained_label}_accuracy'])\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_auroc', output[f'{fine_grained_label}_auroc'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_precision', output[f'{fine_grained_label}_precision'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_recall', output[f'{fine_grained_label}_recall'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_f1', output[f'{fine_grained_label}_f1'])\n",
    "\n",
    "            self.log(f'val/super_loss', output['super_loss'])\n",
    "            self.log(f'val/super_accuracy', output['super_accuracy'])\n",
    "            self.log(f'val/super_auroc', output['super_auroc'])\n",
    "\n",
    "        self.validation_step_outputs.append(output['auroc'])\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self.common_step(batch, batch_idx, calling_function='validation')\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            image_loss = output['image_loss']\n",
    "        else:\n",
    "            image_loss = 0\n",
    "\n",
    "        if self.weight_text_loss > 0:\n",
    "            text_loss = output['text_loss']\n",
    "        else:\n",
    "            text_loss = 0\n",
    "\n",
    "        if self.fine_grained_labels and self.outputs_fine_grained:\n",
    "            fine_grained_loss = torch.mean(torch.Tensor([output[f'{fine_grained_label}_loss'] for fine_grained_label in self.fine_grained_labels]))\n",
    "            super_loss = output['super_loss']\n",
    "        else:\n",
    "            fine_grained_loss = 0.0\n",
    "            super_loss = 0.0\n",
    "\n",
    "        total_loss = output['loss'] + self.weight_image_loss * image_loss + self.weight_text_loss * text_loss + self.weight_fine_grained_loss * fine_grained_loss + self.weight_super_loss * super_loss\n",
    "\n",
    "        self.log(f'val/total_loss', total_loss)\n",
    "        self.log(f'val/loss', output['loss'])\n",
    "        self.log(f'val/accuracy', output['accuracy'])\n",
    "        self.log(f'val/auroc', output['auroc'])\n",
    "\n",
    "        if self.weight_image_loss > 0:\n",
    "            self.log(f'val/image_loss', image_loss)\n",
    "        if self.weight_text_loss > 0:\n",
    "            self.log(f'val/text_loss', text_loss)\n",
    "\n",
    "\n",
    "        # TODO include this logic if needed\n",
    "        if self.fine_grained_labels and self.compute_fine_grained_metrics:\n",
    "            self.log(f'val/fine_grained_loss', fine_grained_loss)\n",
    "            self.log(f'val/super_loss', super_loss)\n",
    "\n",
    "            for fine_grained_label in self.fine_grained_labels:\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_accuracy', output[f'{fine_grained_label}_accuracy'])\n",
    "                self.log(f'val-fine-grained/{fine_grained_label}_auroc', output[f'{fine_grained_label}_auroc'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_precision', output[f'{fine_grained_label}_precision'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_recall', output[f'{fine_grained_label}_recall'])\n",
    "                # self.log(f'val-fine-grained/{fine_grained_label}_f1', output[f'{fine_grained_label}_f1'])\n",
    "\n",
    "            self.log(f'val/super_loss', output['super_loss'])\n",
    "            self.log(f'val/super_accuracy', output['super_accuracy'])\n",
    "            self.log(f'val/super_auroc', output['super_auroc'])\n",
    "\n",
    "        self.test_step_outputs.append(output['auroc'])\n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "\n",
    "    # def on_train_epoch_end(self, validation_step_outputs):\n",
    "    def on_train_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "    # def on_validation_epoch_end(self, validation_step_outputs):\n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "\n",
    "        epoch_average = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.log(\"validation_epoch_average\", epoch_average)\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.acc.reset()\n",
    "        self.auroc.reset()\n",
    "        # self.precision_score.reset()\n",
    "        # self.recall.reset()\n",
    "        # self.f1.reset()\n",
    "        print(\"what is data: \", self.test_step_outputs)\n",
    "\n",
    "        epoch_average = torch.stack(self.test_step_outputs).mean()\n",
    "        self.log(\"test_epoch_average\", epoch_average)\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "\n",
    "    # def test_epoch_end(self, validation_step_outputs):\n",
    "    #     self.acc.reset()\n",
    "    #     self.auroc.reset()\n",
    "    #     self.precision_score.reset()\n",
    "    #     self.recall.reset()\n",
    "    #     self.f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if p.requires_grad]}\n",
    "            ]\n",
    "        # print(\"what are params \", param_dicts)\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def create_model(args, fine_grained_labels):\n",
    "    compute_fine_grained_metrics = True\n",
    "    model = CLIPClassifier(args=args, fine_grained_labels=fine_grained_labels, compute_fine_grained_metrics = compute_fine_grained_metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "23be74d9-efa7-41dc-a23a-c1af110f277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup params\n",
    "clip_model = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "default_param = {\n",
    "    \"use_pretrained_map\": False,\n",
    "    \"num_mapping_layers\": 1,\n",
    "    \"map_dim\": 768,\n",
    "    \"fusion\": \"align\",\n",
    "    \"num_pre_output_layers\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"weight_image_loss\": 0,\n",
    "    \"weight_text_loss\": 0,\n",
    "    \"weight_fine_grained_loss\": 0,\n",
    "    \"weight_super_loss\": 0,\n",
    "    \"fine_grained_labels\": [],\n",
    "    \"clip_pretrained_model\": clip_model,\n",
    "    \"drop_probs\": 0.1,\n",
    "    \"freeze_image_encoder\": True,\n",
    "    \"freeze_text_encoder\": True\n",
    "}\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "\n",
    "model = create_model(default_param, fine_grained_labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "4b40b823-a22b-4803-aea3-7629ebb13664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPClassifier(\n",
       "  (acc): BinaryAccuracy()\n",
       "  (auroc): BinaryAUROC()\n",
       "  (image_encoder): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (image_map): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (text_map): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=768, bias=True)\n",
       "  )\n",
       "  (pre_output): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=784, out_features=768, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_image): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (output_text): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (cross_entropy_loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "4cddf080-0c0d-45da-86fc-499d7223928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# monitor=\"val/auroc\"\n",
    "# project=\"meme-v2\"\n",
    "\n",
    "# checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename=project,  monitor=monitor, \n",
    "#                                       mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "9a80e2a0-5d6c-48ad-b65b-ba9bc8cbf57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "seed_everything(42)\n",
    "model = create_model(default_param, fine_grained_labels=[])\n",
    "\n",
    "\n",
    "# #TODO add GPU later\n",
    "# gpus=args.gpus\n",
    "\n",
    "max_steps = -1\n",
    "gradient_clip_val = 0.1\n",
    "log_every_n_steps = 50\n",
    "max_epochs = -1\n",
    "val_check_interval = 1.0\n",
    "limit_train_batches = 1.0\n",
    "limit_val_batches = 1.0\n",
    "\n",
    "monitor=\"val/auroc\"\n",
    "project=\"meme-v2\"\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints', filename='checkpointFile',  \n",
    "                                      monitor=monitor, mode='max', verbose=True, save_weights_only=True, save_top_k=3, save_last=False)\n",
    "\n",
    "# accelerator=\"cpu\", devices=2\n",
    "# accelerator=\"gpu\", devices=1\n",
    "\n",
    "trainer = Trainer(max_epochs=max_epochs, max_steps=max_steps, gradient_clip_val=gradient_clip_val, \n",
    "        log_every_n_steps=log_every_n_steps, val_check_interval=val_check_interval, accelerator=\"cpu\", devices=1,\n",
    "        strategy=\"auto\", callbacks=[checkpoint_callback],\n",
    "        limit_train_batches=limit_train_batches, limit_val_batches=limit_val_batches,\n",
    "        deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "7eebd455-0b16-4c0a-a44a-dbca8e9566f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:639: Checkpoint directory checkpoints exists and is not empty.\n",
      "\n",
      "   | Name               | Type                  | Params\n",
      "--------------------------------------------------------------\n",
      "0  | acc                | BinaryAccuracy        | 0     \n",
      "1  | auroc              | BinaryAUROC           | 0     \n",
      "2  | image_encoder      | CLIPVisionTransformer | 87.5 M\n",
      "3  | text_encoder       | CLIPTextTransformer   | 63.2 M\n",
      "4  | image_map          | Sequential            | 787 K \n",
      "5  | text_map           | Sequential            | 656 K \n",
      "6  | pre_output         | Sequential            | 602 K \n",
      "7  | output             | Linear                | 769   \n",
      "8  | output_image       | Linear                | 769   \n",
      "9  | output_text        | Linear                | 769   \n",
      "10 | cross_entropy_loss | BCEWithLogitsLoss     | 0     \n",
      "--------------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "150 M     Non-trainable params\n",
      "152 M     Total params\n",
      "610.682   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                               | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  23%|âââââââââââââ                                            | 30/133 [25:23:43<87:11:26,  0.00it/s, v_num=9]\n",
      "Sanity Checking DataLoader 0:  50%|âââââââââââââââââââââââââââ                           | 1/2 [00:02<00:02,  0.44it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  16%|ââââââââââ                                                    | 83/532 [05:42<30:50,  0.24it/s, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D135\\Anaconda3\\envs\\hateclipe\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc65d5-3f56-4a7c-9dcf-7c4a4bcf0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the checkpoints file name from the best stored hyper-params\n",
    "\n",
    "checkpoint = 'checkpoints/checkpointFile-v2.ckpt'\n",
    "trainer.test(ckpt_path=checkpoint, dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b45cd-3c24-4cf6-83c6-556e0f7d50e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505607b-c204-4047-8376-5a3f859c3762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b73bd44-dbb2-4f9a-98ec-7d55662cccd0",
   "metadata": {},
   "source": [
    "- Added 0 worker for dataloader and 1 device for trainer. The auroc is 0.448 and it is unchanged through different epochs.\n",
    "- <the issue could be that configure_optimizer is not taking the right parameters and updating the right thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2b165-662a-428d-8e24-0d397cb94de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
